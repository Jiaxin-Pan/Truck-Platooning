{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Pipeline\n",
    "\n",
    "From the first notebook for data collection, we have collected different datasets with different approaches. Now it's time to train our models with collected datasets. In this notebook we will go through the training process of different models. The training process is generally divided into four steps, data preparation, building the model, training the model and test inference.\n",
    "\n",
    "**HINTS:**\n",
    "\n",
    "Before running all the following cells for training, please place **the folder ‘model’ as well as this notebook Training_Pipeling.ipynb** under path <code>/storage/remote/atcremers51/truck_platooning</code>. \n",
    "\n",
    "Then we can start the training process in following cells. \n",
    "\n",
    "**The models to be trained and their corresponding datasets are listed as follows,**\n",
    "1. MLP(Multi-layer Perceptron) model trained with raw states\n",
    "2. MLP model trained with relative transformation\n",
    "3. FCNN(Fully Connected Neural Network) model trained end-to-end with CARLA online dataset\n",
    "4. Two-step CNN(Convolutional Neural Network)-MLP model trained with CARLA online dataset\n",
    "5. Two-step CNN-MLP model trained with depth-based dataset\n",
    "6. Two-step CNN-MLP model trained with stereo-based dataset\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader \n",
    "import random\n",
    "import time\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h3>Note: Google Colab</h3>\n",
    "    <p>\n",
    "In case you don't have a GPU, you can run this notebook on Google Colab where you can access a GPU for free, but, of course, you can also run this notebook on your CPU.\n",
    "         </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import datasets for different models\n",
    "Here we used <code>torch.utils.data.Dataset</code> to build custom datasets. All Dataset classes are stored in <code>model/my_datasets.py</code>. The following cells corresponds to the **dataset preparation processes of different models**. If you want to import the dataset for one of the models mentioned above, please run the corresponding cell.\n",
    "\n",
    "All datasets used for training are stored on the remote machine under <code>/storage/remote/atcremers51/turck_platooning/dataset</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/storage/remote/atcremers51/truck_platooning/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) MLP with raw states\n",
    "Here we used the dataset collected directly in CARLA simulator(https://carla.org/) with around 30,000 samples from only one trajectory in town04. The MLP model trained with raw states has 8 inputs (x_ref, y_ref, yaw_ref, v_ref, x_ego, y_ego, yaw_ego, v_ego) and 2 outputs (throttle, steering angle). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.my_datasets import MyDataset_MLP \n",
    "from dataset.MLP_raw_dataset.model_input_all import state_on, input_0, input_4, input_5, input_6\n",
    "from dataset.MLP_raw_dataset.model_gt_all import label_on,label_0, label_4, label_5, label_6\n",
    "\n",
    "# gather all data together\n",
    "input_all = np.concatenate((state_on, input_0, input_4, input_5, input_6), axis = 0) \n",
    "label_all = np.concatenate((label_on,label_0, label_4, label_5, label_6), axis = 0) \n",
    "data_all = np.concatenate((input_all, label_all), axis = 1) \n",
    "np.random.shuffle(data_all)\n",
    "print('Number of all samples:', data_all.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train dataset, validation dataset, and test dataset\n",
    "def split_dataset(data, split_list):\n",
    "    split_num = data.shape[0] * np.array(split_list)\n",
    "    return data[:round(split_num[0]), :], data[round(split_num[0]):round(split_num[1]+split_num[0]), :], data[round(split_num[1]+split_num[0]):, :]\n",
    "\n",
    "split_list = [0.7, 0.2, 0.1]\n",
    "train_input, val_input, test_input = split_dataset(data_all[:, :8], split_list)\n",
    "train_label, val_label, test_label = split_dataset(data_all[:, 8:], split_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over_data with only two samples to test if the model can overfit at the beginning. \n",
    "over_input = data_all[:2, :8]\n",
    "over_label = data_all[:2, 8:]\n",
    "\n",
    "train_data = MyDataset_MLP(states = train_input, labels = train_label)\n",
    "val_data = MyDataset_MLP(states = val_input, labels = val_label)\n",
    "test_data = MyDataset_MLP(states = test_input, labels = test_label)\n",
    "over_data = MyDataset_MLP(states = over_input, labels = over_label)   \n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=True)\n",
    "over_loader = DataLoader(dataset=over_data, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Test size: %i\" % len(test_data))\n",
    "print(\"Input: [x_ref, y_ref, yaw_ref, v_ref, x_ego, y_ego, yaw_ego, v_ego]\")\n",
    "print('Input Example:', train_data[0][0])\n",
    "print(\"Label: [throttle, steering angle]\")\n",
    "print('Label Example:', train_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) MLP with relative states\n",
    "Here we used the dataset collected directly in CARLA simulator with around 60,000 samples from two trajectories in town04. The MLP model trained with raw states has 5 inputs (delta_x, delta_y, delta_yaw, v_ref, v_ego) and 2 outputs (throttle, steering angle). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.carla_online_dataset.data_transform_array import data_transform \n",
    "from model.my_datasets import MyDataset_MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train dataset, validation dataset, and test dataset\n",
    "def split_dataset(data, split_list): \n",
    "    split_num = data.shape[0] * np.array(split_list) \n",
    "    return data[:round(split_num[0]), :], data[round(split_num[0]):round(split_num[1]+split_num[0]), :], data[round(split_num[1]+split_num[0]):, :]\n",
    " \n",
    "np.random.shuffle(data_transform) \n",
    "split_list = [0.8, 0.1, 0.1]\n",
    "train_input, val_input, test_input = split_dataset(data_transform[:, :5], split_list)\n",
    "train_label, val_label, test_label = split_dataset(data_transform[:, 5:], split_list) \n",
    "print('Number of all samples:', data_transform.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over_data with only two samples to test if the model can overfit at the beginning. \n",
    "over_input = data_transform[:2, :5]\n",
    "over_label = data_transform[:2, 5:]\n",
    "\n",
    "train_data = MyDataset_MLP(states = train_input, labels = train_label)\n",
    "val_data = MyDataset_MLP(states = val_input, labels = val_label)\n",
    "test_data = MyDataset_MLP(states = test_input, labels = test_label)\n",
    "over_data = MyDataset_MLP(states = over_input, labels = over_label)   \n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=128, shuffle=True)\n",
    "over_loader = DataLoader(dataset=over_data, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Test size: %i\" % len(test_data)) \n",
    "print('Input: [delta_x, delta_y, delta_yaw, v_ref, v_ego]')\n",
    "print('Input Example:', train_data[0][0])\n",
    "print('Label: [throttle, steering angle]')\n",
    "print('Label Example:', train_data[0][1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) End-to-end FCNN\n",
    "Here we used the dataset collected directly in CARLA simulator with around 60,000 samples from two trajectories in town04. The FCNN model has 3 inputs (RGB image, v_ref, v_ego) and 2 outputs (throttle, steering angle).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.my_datasets import MyDataset_FCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train dataset, validation dataset, and test dataset \n",
    "# over_data with only 4 samples to test if the model can overfit at the beginning.\n",
    "ROOT = './dataset/carla_online_dataset/' \n",
    "train_out = open(ROOT+\"traindata_shuffle_fcnn.txt\",'w')\n",
    "val_out = open(ROOT+\"valdata_shuffle_fcnn.txt\",'w')\n",
    "over_out = open(ROOT+\"overdata_shuffle_fcnn.txt\",'w')\n",
    "lines=[]\n",
    "with open(ROOT+\"data_all.txt\", 'r') as infile:\n",
    "    for line in infile:\n",
    "        lines.append(line)\n",
    "    random.shuffle(lines)\n",
    "    num_train = np.ceil(0.8*len(lines))\n",
    "    for count, line in enumerate(lines):\n",
    "        if count <=num_train:\n",
    "            if count< 4:\n",
    "                over_out.write(line)\n",
    "            train_out.write(line)\n",
    "        else:\n",
    "            val_out.write(line) \n",
    "train_out.close()            \n",
    "val_out.close()\n",
    "over_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (128,128)\n",
    "TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "\n",
    "\n",
    "over_data = MyDataset_CNN(root = ROOT, txtname = 'overdata_shuffle_fcnn.txt', transform=TRANSFORM, size=INPUT_SIZE)\n",
    "train_data = MyDataset_CNN(root = ROOT, txtname = 'traindata_shuffle_fcnn.txt' , transform = TRANSFORM, size= INPUT_SIZE)\n",
    "val_data = MyDataset_CNN(root = ROOT, txtname = 'valdata_shuffle_fcnn.txt', transform = TRANSFORM, size= INPUT_SIZE)\n",
    "\n",
    "over_loader = DataLoader(dataset=over_data, batch_size=4, shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4) \n",
    "\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Overfit size: %i\" % len(over_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) CNN-MLP model trained with CARLA online dataset\n",
    "Here we used the dataset collected directly in CARLA simulator with around 60,000 samples from two trajectories in town04. \n",
    "\n",
    "The MLP model is trained before in section 3. Here our goal is to train a CNN to predict relative transformation. The CNN has 3 inputs (RGB image, v_ref, v_ego) and 3 outputs (delta_x, delta_y, delta_yaw).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.my_datasets import MyDataset_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train dataset, validation dataset, and test dataset \n",
    "# over_data with only 4 samples to test if the model can overfit at the beginning.\n",
    "ROOT = './dataset/carla_online_dataset/'  \n",
    "train_out = open(ROOT+\"traindata_shuffle_cnn.txt\",'w')\n",
    "val_out = open(ROOT+\"valdata_shuffle_cnn.txt\",'w')\n",
    "over_out = open(ROOT+\"overdata_shuffle_cnn.txt\",'w')\n",
    "lines=[]\n",
    "with open(ROOT+\"data_transformation.txt\", 'r') as infile:\n",
    "    for line in infile:\n",
    "        lines.append(line)\n",
    "    random.shuffle(lines)\n",
    "    num_train = np.ceil(0.8*len(lines))\n",
    "    for count, line in enumerate(lines):\n",
    "        if count <=num_train:\n",
    "            if count< 4:\n",
    "                over_out.write(line)\n",
    "            train_out.write(line)\n",
    "        else:\n",
    "            val_out.write(line) \n",
    "train_out.close()            \n",
    "val_out.close()\n",
    "over_out.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (128,128)\n",
    "TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "\n",
    "\n",
    "over_data = MyDataset_CNN(root = ROOT, txtname = 'overdata_shuffle_cnn.txt', transform=TRANSFORM, size=INPUT_SIZE)\n",
    "train_data = MyDataset_CNN(root = ROOT, txtname = 'traindata_shuffle_cnn.txt' , transform = TRANSFORM, size= INPUT_SIZE)\n",
    "val_data = MyDataset_CNN(root = ROOT, txtname = 'valdata_shuffle_cnn.txt', transform = TRANSFORM, size= INPUT_SIZE)\n",
    "\n",
    "over_loader = DataLoader(dataset=over_data, batch_size=4, shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4) \n",
    "\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Overfit size: %i\" % len(over_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input image size:',over_data[0][0][0].shape)\n",
    "print('Input v_ref and v_ego: %.3f m/s, %.3f m/s' %(over_data[0][0][1], over_data[0][0][2]))\n",
    "print('Labels (delta_x, delta_y, delta_yaw):', over_data[0][1])\n",
    "plt.imshow(over_data[0][0][0].view(128,128,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) CNN-MLP model trained with depth-based dataset\n",
    "Here we used the dataset collected through image rendering. It contains around 45,000 samples. The on-trajectory data is directly collected from CARLA simulator. And the off-trajectory data is generated with **the depth map taken by depth camera in CARLA simulator**. \n",
    "\n",
    "The MLP model is trained before in section 3. Here our goal is to train a CNN to predict relative transformation. The CNN has 3 inputs (RGB image, v_ref, v_ego) and 3 outputs (delta_x, delta_y, delta_yaw).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.my_datasets import MyDataset_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train dataset, validation dataset, and test dataset \n",
    "# over_data with only 4 samples to test if the model can overfit at the beginning. \n",
    "ROOT = './dataset/depth_dataset/' \n",
    "train_out = open(ROOT+\"traindata_shuffle_depth.txt\",'w')\n",
    "val_out = open(ROOT+\"valdata_shuffle_depth.txt\",'w')\n",
    "over_out = open(ROOT+\"overdata_shuffle_depth.txt\",'w')\n",
    "lines=[]\n",
    "with open(ROOT+\"delta_data.txt\", 'r') as infile:\n",
    "    for line in infile:\n",
    "        lines.append(line)\n",
    "    random.shuffle(lines)\n",
    "    num_train = np.ceil(0.8*len(lines))\n",
    "    for count, line in enumerate(lines):\n",
    "        if count <=num_train:\n",
    "            if count< 4:\n",
    "                over_out.write(line)\n",
    "            train_out.write(line)\n",
    "        else:\n",
    "            val_out.write(line) \n",
    "train_out.close()            \n",
    "val_out.close()\n",
    "over_out.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (128,128)\n",
    "TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "\n",
    "\n",
    "over_data = MyDataset_CNN(root = ROOT, txtname = 'overdata_shuffle_depth.txt', transform=TRANSFORM, size=INPUT_SIZE)\n",
    "train_data = MyDataset_CNN(root = ROOT, txtname = 'traindata_shuffle_depth.txt' , transform = TRANSFORM, size= INPUT_SIZE)\n",
    "val_data = MyDataset_CNN(root = ROOT, txtname = 'valdata_shuffle_depth.txt', transform = TRANSFORM, size= INPUT_SIZE)\n",
    "\n",
    "over_loader = DataLoader(dataset=over_data, batch_size=4, shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4) \n",
    "\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Overfit size: %i\" % len(over_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input image size:',over_data[0][0][0].shape)\n",
    "print('Input v_ref and v_ego: %.3f m/s, %.3f m/s' %(over_data[0][0][1], over_data[0][0][2]))\n",
    "print('Labels (delta_x, delta_y, delta_yaw):', over_data[0][1])\n",
    "plt.imshow(over_data[0][0][0].view(128,128,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) CNN-MLP model trained with stereo-based dataset\n",
    "Here we used the dataset collected through image rendering. It contains around 40,000 samples. The on-trajectory data is directly collected from CARLA simulator. And the off-trajectory data is generated with **estimated depth map based on stereo vision system**. \n",
    "\n",
    "The MLP model is trained before in section 3. Here our goal is to train a CNN to predict relative transformation. The CNN has 3 inputs (RGB image, v_ref, v_ego) and 3 outputs (delta_x, delta_y, delta_yaw).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.my_datasets import MyDataset_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train dataset, validation dataset, and test dataset \n",
    "# over_data with only 4 samples to test if the model can overfit at the beginning.\n",
    "ROOT = './dataset/stereo_dataset/'  \n",
    "train_out = open(ROOT+\"traindata_shuffle_stereo.txt\",'w')\n",
    "val_out = open(ROOT+\"valdata_shuffle_stereo.txt\",'w')\n",
    "over_out = open(ROOT+\"overdata_shuffle_stereo.txt\",'w')\n",
    "lines=[]\n",
    "with open(ROOT+\"delta_data.txt\", 'r') as infile:\n",
    "    for line in infile:\n",
    "        lines.append(line)\n",
    "    random.shuffle(lines)\n",
    "    num_train = np.ceil(0.8*len(lines))\n",
    "    for count, line in enumerate(lines):\n",
    "        if count <=num_train:\n",
    "            if count< 4:\n",
    "                over_out.write(line)\n",
    "            train_out.write(line)\n",
    "        else:\n",
    "            val_out.write(line) \n",
    "train_out.close()            \n",
    "val_out.close()\n",
    "over_out.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (128,128)\n",
    "TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "\n",
    "\n",
    "over_data = MyDataset_CNN(root = ROOT, txtname = 'overdata_shuffle_depth.txt', transform=TRANSFORM, size=INPUT_SIZE)\n",
    "train_data = MyDataset_CNN(root = ROOT, txtname = 'traindata_shuffle_depth.txt' , transform = TRANSFORM, size= INPUT_SIZE)\n",
    "val_data = MyDataset_CNN(root = ROOT, txtname = 'valdata_shuffle_depth.txt', transform = TRANSFORM, size= INPUT_SIZE)\n",
    "\n",
    "over_loader = DataLoader(dataset=over_data, batch_size=4, shuffle=True)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=256, shuffle=True, pin_memory=True, num_workers = 4) \n",
    "\n",
    "\n",
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Overfit size: %i\" % len(over_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input image size:',over_data[0][0][0].shape)\n",
    "print('Input v_ref and v_ego: %.3f m/s, %.3f m/s' %(over_data[0][0][1], over_data[0][0][2]))\n",
    "print('Labels (delta_x, delta_y, delta_yaw):', over_data[0][1])\n",
    "plt.imshow(over_data[0][0][0].view(128,128,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the model\n",
    "To train different models, we first define different model architectures inherited from <code>torch.nn.Module</code> in <code>model/my_models.py</code>. The following cells show **the process of model imports and model building**. If you want to define one of the models mentioned above, please run the corresponding cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1) MLP with raw states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.my_models import MyModel_MLP_raw\n",
    "neurons = [256, 1024, 2048, 1024, 256]\n",
    "mynet = MyModel_MLP_raw(neurons)\n",
    "mynet.to(device)\n",
    "print(mynet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2) MLP with relative states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.my_models import MyModel_MLP_transform\n",
    "neurons = [256, 1024, 256]\n",
    "mynet = MyModel_MLP_transform(neurons)\n",
    "mynet.to(device)\n",
    "print(mynet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3)  End-to-end FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.my_models import MyModel_FCNN_endtoend\n",
    "mynet = MyModel_FCNN_endtoend()\n",
    "mynet.to(device)\n",
    "print(mynet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4) 5) 6)  CNN-MLP\n",
    "The three CNN-MLP models trained with different datasets have the same architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from model.my_models import MyModel_CNN_relative\n",
    "mynet = MyModel_CNN_relative()\n",
    "mynet.to(device)\n",
    "print(mynet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Check the number of trainable parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in mynet.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in mynet.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycriterion = nn.MSELoss(reduction='mean') \n",
    "myoptimizer = optim.Adam(mynet.parameters(), lr=1e-4, eps = 1e-08) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1）2）MLP with raw states / relative states\n",
    "\n",
    "To start training, we should first use a small dataset(over_data) to test if our model can overfit with such small dataset. If so, our model is reasonable and we can start training. Then we can start to train the model. After training for hundreds of epochs, the trained model will be saved. At last, we test the model inference with 50 samples in test dataset to see how well the model has been trained and how large the difference between the prediction and ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model,criterion,optimizer,dataloader,iftrain):    \n",
    "    running_loss = 0.0  \n",
    "     \n",
    "    for i, data in enumerate(dataloader, 0): \n",
    "        X, y = data\n",
    "        X = X.to(device)\n",
    "        y = y.to(device) \n",
    "        if iftrain:   \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X.to(torch.float32)) \n",
    "            y_pred = y_pred.float()\n",
    "            y = y.float() \n",
    "            \n",
    "            loss = criterion(y_pred, y)  \n",
    "            loss.backward()             \n",
    "            optimizer.step()            \n",
    "            running_loss += loss.item() \n",
    "        else:\n",
    "            model.eval()\n",
    "            y_pred = model(X.to(torch.float32))\n",
    "            y_pred = y_pred.float()  \n",
    "            y = y.float() \n",
    "            loss = criterion(y_pred , y)  \n",
    "            running_loss += loss.item()    \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the model will overfit with small dataset\n",
    "max_epochs = 1000 \n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(max_epochs): \n",
    "    train_loss = run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=over_loader,\n",
    "                           iftrain=True)\n",
    "    train_history.append(train_loss)\n",
    "    val_loss =  run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=over_loader,\n",
    "                           iftrain=False)\n",
    "    val_history.append(val_loss)\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train_loss: %2e, val_loss: %2e \"% \n",
    "          (train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the main training process\n",
    "max_epochs = 1000\n",
    "num_trainbatch = np.ceil(len(train_data)/256) \n",
    "num_valbatch = np.ceil(len(val_data)/256) \n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "print('Start training!')\n",
    "\n",
    "for epoch in range(max_epochs): \n",
    "    train_loss = run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=train_loader,\n",
    "                           iftrain=True)\n",
    "    train_history.append(train_loss/num_trainbatch)\n",
    "    val_loss =  run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=val_loader,\n",
    "                           iftrain=False)\n",
    "    val_history.append(val_loss/num_valbatch)\n",
    "    if epoch % 100 == 99:\n",
    "      print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train_loss: %e, val_loss: %e \"% \n",
    "          (train_loss/num_trainbatch, val_loss/num_valbatch)) \n",
    "print('Finish training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(len(train_history))\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_history, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_history, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model.state_dict(),'./model/best_model_MLP.pth')\n",
    "print('MLP Model saved！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference \n",
    "for n, data in enumerate(test_loader): \n",
    "        X = torch.tensor(data[0]).to(device) \n",
    "        y = torch.tensor(data[1]).to(device)\n",
    "        if n < 50:\n",
    "            for i, x in enumerate(X): \n",
    "            mynet.eval()\n",
    "            x = x[None, :]\n",
    "            y_pred = mynet(x.to(torch.float32))  \n",
    "            print('prediction  : {},\\nground truth：{}'.format(y_pred, y[i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3) End-to-end FCNN\n",
    "\n",
    "To start training, we should first use a small dataset(over_data) to test if our model can overfit with such small dataset. If so, our model is reasonable and we can start training. Then we can start to train the model. After training for hundreds of epochs, the trained model will be saved. At last, we test the model inference with 50 samples in test dataset to see how well the model has been trained and how large the difference between the prediction and ground truth.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(model,criterion,optimizer,dataloader,iftrain):    \n",
    "    running_loss = 0.0   \n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0): \n",
    "        X, y = data \n",
    "        X = [x.to(device) for x in X] \n",
    "        y = y.to(device) \n",
    "        \n",
    "        if iftrain:  \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X) \n",
    "            y_pred = y_pred.float() \n",
    "            y = y.float() \n",
    "            loss = criterion(y_pred, y) \n",
    "            loss.backward()             \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()              \n",
    "            \n",
    "        else:\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.float()  \n",
    "            y = y.float() \n",
    "            loss = criterion(y_pred, y)  \n",
    "            running_loss += loss.item()      \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test if the model will overfit with small dataset\n",
    "max_epochs = 1000 \n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(max_epochs): \n",
    "    train_loss = run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=over_loader,\n",
    "                           iftrain=True)\n",
    "    train_history.append(train_loss)\n",
    "    val_loss =  run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=over_loader,\n",
    "                           iftrain=False)\n",
    "    val_history.append(val_loss)\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train_loss: %2e, val_loss: %2e \"% \n",
    "          (train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start the main training process\n",
    "max_epochs = 500\n",
    "num_trainbatch = np.ceil(len(train_data)/256) \n",
    "num_valbatch = np.ceil(len(val_data)/256) \n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "print('Start training!')\n",
    "\n",
    "for epoch in range(max_epochs): \n",
    "    train_loss = run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=train_loader,\n",
    "                           iftrain=True)\n",
    "    train_history.append(train_loss/num_trainbatch)\n",
    "    val_loss =  run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=val_loader,\n",
    "                           iftrain=False)\n",
    "    val_history.append(val_loss/num_valbatch)\n",
    "    if epoch % 100 == 99:\n",
    "      print(f\"Epoch {epoch + 1: >3}/{max_epochs}, train_loss: %e, val_loss: %e \"% \n",
    "          (train_loss/num_trainbatch, val_loss/num_valbatch)) \n",
    "print('Finish training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = range(len(train_history))\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_history, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_history, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model.state_dict(),'./model/best_model_FCNN.pth')\n",
    "print('FCNN Model saved！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test inference\n",
    "for i in range(50):\n",
    "    test_num = random.randint (0, 100)\n",
    "    test_in, gt_output = val_data[test_num]\n",
    "    test_in = [torch.tensor(x).to(device) for x in test_in] \n",
    "    gt_output = torch.tensor(gt_output).to(device)\n",
    "    output_pred = mynet(test_in) \n",
    "    print('prediction:{}, ground truth:{}'.format(output_pred.cpu().detach().numpy(), gt_output.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####  4) 5) 6)  CNN-MLP\n",
    "The three CNN-MLP models trained with different datasets have the same training process. The three outputs(delta_x, delta_y, delta_yaw) have different scales. delta_y and delta_yaw always have extremely small values. Thus, to obtain a gooe training result, it's necessary to **observe the loss of these three outputs separately.**\n",
    "\n",
    "To start training, we should first use a small dataset(over_data) to test if our model can overfit with such small dataset. If so, our model is reasonable and we can start training. Then we can start to train the model. In order to avoid overfitting, we used the **early stop strategy**. If the validation loss doesn't fall in 10 epochs, the training process will be stopped. And the trained model with the lowest validation loss will be saved as the best model. At last, we test the model inference with 50 samples in test dataset to see how well the model has been trained and how large the difference between the prediction and ground truth.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(model,criterion,optimizer,dataloader,iftrain):    \n",
    "    running_loss = 0.0 \n",
    "    running_loss0 = 0.0\n",
    "    running_loss1 = 0.0\n",
    "    running_loss2 = 0.0 \n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0): \n",
    "        X, y = data \n",
    "        X = [x.to(device) for x in X] \n",
    "        y = y.to(device) \n",
    "        \n",
    "        if iftrain:  \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X) \n",
    "            y_pred = y_pred.float() \n",
    "            y = y.float() \n",
    "            loss0 = criterion(y_pred[:,0], y[:,0])\n",
    "            loss1 = criterion(y_pred[:,1], y[:,1])\n",
    "            loss2 = criterion(y_pred[:,2], y[:,2]) \n",
    "            loss = (loss0 + loss1 + loss2)/3   \n",
    "            loss.backward()             \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_loss0 += loss0.item() \n",
    "            running_loss1 += loss1.item()\n",
    "            running_loss2 += loss2.item()             \n",
    "            \n",
    "        else:\n",
    "            y_pred = model(X)\n",
    "            y_pred = y_pred.float()  \n",
    "            y = y.float() \n",
    "            loss0 = criterion(y_pred[:,0], y[:,0])\n",
    "            loss1 = criterion(y_pred[:,1], y[:,1])\n",
    "            loss2 = criterion(y_pred[:,2], y[:,2])  \n",
    "            loss = (loss0 + loss1 + loss2)/3\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_loss0 += loss0.item() \n",
    "            running_loss1 += loss1.item()\n",
    "            running_loss2 += loss2.item()      \n",
    "    return running_loss, running_loss0, running_loss1, running_loss2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test if the model will overfit with small dataset\n",
    "max_epochs = 1000\n",
    "train_history = []\n",
    "val_history = []  \n",
    "for epoch in range(max_epochs): \n",
    "    start = time.time()\n",
    "    train_loss,_,_,_ = run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=over_loader,\n",
    "                           iftrain=True)\n",
    "    train_history.append(train_loss)\n",
    "    val_loss, val_loss0, val_loss1, val_loss2 =  run_epoch(model=mynet,criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=over_loader,\n",
    "                           iftrain=False)\n",
    "    val_history.append(val_loss)\n",
    "    end = time.time()\n",
    "    if epoch % 50 == 49:\n",
    "        print('Epoch %.0f / 1000, train_loss: %2e, val_loss: %2e(%2e,%2e,%2e), runtime:% s' % \n",
    "              (epoch+1, train_loss, val_loss, val_loss0, val_loss1, val_loss2, end-start))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# start the main training process\n",
    "num_trainbatch = np.ceil(len(train_data)/256) \n",
    "num_valbatch = np.ceil(len(val_data)/256) \n",
    "train_history = []\n",
    "val_history = []\n",
    "min_loss = 100000\n",
    "current_patience = 0\n",
    "patience = 10\n",
    "best_model = MyModel_CNN_relative()\n",
    "best_model.to(device)\n",
    "max_epochs = 100\n",
    "\n",
    "print('Start training!')\n",
    "for epoch in range(max_epochs): \n",
    "    start = time.time()\n",
    "    train_loss,_,_,_ = run_epoch(model=mynet, criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=train_loader,\n",
    "                           iftrain=True)\n",
    "    train_history.append(train_loss)\n",
    "    val_loss, val_loss0, val_loss1, val_loss2 =  run_epoch(model=mynet, criterion=mycriterion,\n",
    "                           optimizer=myoptimizer,dataloader=val_loader,\n",
    "                           iftrain=False)\n",
    "    val_history.append(val_loss)\n",
    "    end = time.time()\n",
    "    if epoch % 10 == 9:\n",
    "    print('Epoch %.0f / %.0f, train_loss: %2e, val_loss: %2e(%2e,%2e,%2e), runtime:% s' % \n",
    "              (epoch+1, max_epochs, train_loss/num_trainbatch, val_loss/num_valbatch, val_loss0/num_valbatch, val_loss1/num_valbatch, val_loss2/num_valbatch, end-start))\n",
    "\n",
    "\n",
    "    # early stopping\n",
    "    if min_loss == 100000 or val_loss < min_loss :\n",
    "        min_loss = val_loss\n",
    "        current_patience = 0\n",
    "        best_model = mynet \n",
    "      \n",
    "    else :\n",
    "        current_patience += 1 \n",
    "        if current_patience >= patience :\n",
    "            print(\"Stopping early at epoch {}!\".format(epoch+1)) \n",
    "            break   \n",
    "\n",
    "print('Finish training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot the train & validation loss curve with small dataset\n",
    "epochs = range(len(train_history))\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_history, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_history, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save the best model\n",
    "torch.save(best_model.state_dict(),'./model/best_model_CNN.pth')\n",
    "print('Model saved！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test inference\n",
    "for i in range(50):\n",
    "    test_num = random.randint (0, 100)\n",
    "    test_in, gt_output = val_data[test_num]\n",
    "    test_in = [torch.tensor(x).to(device) for x in test_in] \n",
    "    gt_output = torch.tensor(gt_output).to(device)\n",
    "    output_pred = mynet(test_in) \n",
    "    print('prediction:{}, ground truth:{}'.format(output_pred.cpu().detach().numpy(), gt_output.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we go through the entire training process of 6 different models with different dataseets. After training the models, the next step will be to use these models to do inference in the unseen test trajectories in CARLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
